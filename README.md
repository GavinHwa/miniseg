Introduction
============

    MiniSeg在传统的HMM分词模型上做了一点改进，修改了viterbi算法中的状态到字符的发射概率的计算方法，把它改为用Naive Bayes去估计。
    用NB估计的时候考虑了了当前字符周围的字符。经过试验，取得了比传统HMM模型更好的分词效果（有待数据支持）。


Naive Bayes模型的建立
=====================

    NB的feture的建立是5个字符一个窗口。
    比如，对于“....我是中国人....”的“中”这个字。
    它的feature就是：
         (我，是，中，国，人，我是，是中，中国，国人，是国) //10个feature
    它的label就是：
         "B"， 表示”中“这个字是一个词语的开头。
         
    下面是用与NB训练的Feature+Label训练集合的一个截图：
    
      代  中	后	期	，	代中	中后	后期	期，	中期	B
      中	后	期	，	对	中后	后期	期，	，对	后，	E
      后	期	，	对	叔	后期	期，	，对	对叔	期对	S
      期	，	对	叔	本	期，	，对	对叔	叔本	，叔	S
      ，	对	叔	本	华	，对	对叔	叔本	本华	对本	B
      对	叔	本	华	、	对叔	叔本	本华	华、	叔华	M
      叔	本	华	、	尼	叔本	本华	华、	、尼	本、	E
      本	华	、	尼	采	本华	华、	、尼	尼采	华尼	S
      华	、	尼	采	的	华、	、尼	尼采	采的	、采	B
      、	尼	采	的	著	、尼	尼采	采的	的著	尼的	E
      尼	采	的	著	作	尼采	采的	的著	著作	采著	S
      采	的	著	作	读	采的	的著	著作	作读	的作	B
      的	著	作	读	得	的著	著作	作读	读得	著读	E
      著	作	读	得	多	著作	作读	读得	得多	作得	S
      作	读	得	多	了	作读	读得	得多	多了	读多	S
      读	得	多	了	，	读得	得多	多了	了，	得了	S
      得	多	了	，	发	得多	多了	了，	，发	多，	S
      多	了	，	发	现	多了	了，	，发	发现	了发	S
      了	，	发	现	他	了，	，发	发现	现他	，现	B
      ，	发	现	他	们	，发	发现	现他	他们	发他	E
      发	现	他	们	思	发现	现他	他们	们思	现们	B
      现	他	们	思	考	现他	他们	们思	思考	他思	E
      他	们	思	考	的	他们	们思	思考	考的	们考	B
      们	思	考	的	起	们思	思考	考的	的起	思的	E
      思	考	的	起	点	思考	考的	的起	起点	考起	S
      考	的	起	点	也	考的	的起	起点	点也	的点	B
      的	起	点	也	是	的起	起点	点也	也是	起也	E
      起	点	也	是	这	起点	点也	也是	是这	点是	S
             
    你会发现，竖着读是通顺的：），第3列是主线。
    
    B: 词语的开头
    M: 词语的中间
    E: 词语的结束
    S: 单字成词
    
    训练完成后，对于给定的一个字符，我们可以得到5个字符构成的窗口的feature，从而预测出它属于4个lable的概率分别是多少。
    这显然别传统的HMM模型中发射概率：P(char|label)利用了更多的信息。
    
Related Work
==========
TODO

Demo
====
http://miniseg.ap01.aws.af.cm/


